{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Sampling***"
      ],
      "metadata": {
        "id": "1Dxe62vkKmcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reT8rjElKVoh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "def sample_json(input_filename, output_filename, target_size_gb=15, filter_key='also_buy'):\n",
        "    target_size_bytes = target_size_gb * (1024 ** 3)\n",
        "    total_bytes = 0\n",
        "    extracted_gb = 0  # Variable to track extracted gigabytes\n",
        "\n",
        "    with open(input_filename, 'r', encoding='utf-8') as input_file:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "            for line in tqdm.tqdm(input_file):\n",
        "                record = json.loads(line)\n",
        "\n",
        "                # Check if the record satisfies the filter condition\n",
        "                if filter_key is not None and filter_key in record and record[filter_key]:\n",
        "                    json.dump(record, output_file)\n",
        "                    output_file.write('\\n')\n",
        "\n",
        "                    # Calculate the size of the written record and update total bytes\n",
        "                    total_bytes += len(line.encode('utf-8'))\n",
        "\n",
        "                    # Increment the extracted gigabytes counter\n",
        "                    if total_bytes >= target_size_bytes:\n",
        "                        extracted_gb += 1\n",
        "                        print(f\"Currently extracting gb#{extracted_gb}\", end='\\r')\n",
        "\n",
        "    # Display total GB processed after completion\n",
        "    print(f\"Total GB processed: {extracted_gb} GB\")\n",
        "\n",
        "# Example usage:\n",
        "sample_json('E:\\All_Amazon_Meta.json\\All_Amazon_Meta.json', 'sampled_original_amazon.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Uploading the json file***"
      ],
      "metadata": {
        "id": "KC2LGkC9KsLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import json\n",
        "\n",
        "def get_total_records(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        total_records = sum(1 for line in file)\n",
        "    return total_records\n"
      ],
      "metadata": {
        "id": "U7c0Ll4-LaQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pre-processing (I)***"
      ],
      "metadata": {
        "id": "XC7UCQ4eL9oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simplified word tokenization\n",
        "def word_tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Simplified stemming using only the first few characters\n",
        "def stem_word(word):\n",
        "    return word[:4]  # Just take the first 4 characters as a simple stemming approach\n",
        "\n",
        "# Function to read stopwords from a file\n",
        "def read_stopwords_from_file(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        stopwords = [line.strip() for line in file]\n",
        "    return stopwords\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(tokens, stopwords):\n",
        "    return [token for token in tokens if token not in stopwords]\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return ''.join([char for char in text if char not in string.punctuation])\n"
      ],
      "metadata": {
        "id": "aXP8T3RbLYbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pre-processing (II)***"
      ],
      "metadata": {
        "id": "SpN6YuF5MKgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simplified preprocessing function\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing Punctuation\n",
        "    text = remove_punctuation(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing Stopwords\n",
        "    tokens = remove_stopwords(tokens, stopwords)\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_tokens = [stem_word(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(stemmed_tokens)  # Return preprocessed text as a single string"
      ],
      "metadata": {
        "id": "UaLzKXl1LjRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Chunk Processing***"
      ],
      "metadata": {
        "id": "xCxOMFIOMPlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to process data in chunks\n",
        "def process_data_in_chunks(input_file, output_file, total_records, records_per_chunk, stopwords):\n",
        "    line_index = 0\n",
        "    chunk_count = 0\n",
        "    while True:\n",
        "        chunk = []\n",
        "        for _ in range(records_per_chunk):\n",
        "            line = input_file.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            chunk.append(line)\n",
        "            line_index += 1\n",
        "        if not chunk:\n",
        "            break\n",
        "        chunk_count += 1\n",
        "        filtered_chunk = []\n",
        "        for line in chunk:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                # Skip invalid JSON lines\n",
        "                continue\n",
        "            filtered_data = {key: data[key] for key in columns_to_keep if key in data}\n",
        "            # Preprocess text data\n",
        "            for key, value in filtered_data.items():\n",
        "                if isinstance(value, str):\n",
        "                    filtered_data[key] = preprocess_text(value, stopwords)\n",
        "            filtered_chunk.append(filtered_data)\n",
        "        json.dump(filtered_chunk, output_file)\n",
        "        print(f\"Chunk {chunk_count}: {line_index} records filtered. ({(line_index / total_records) * 100:.2f}% done)\")\n"
      ],
      "metadata": {
        "id": "S89AOFRDLkJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Function Calling***"
      ],
      "metadata": {
        "id": "yYz0e9BWMWwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the file paths\n",
        "input_json_path = \"E:\\sampled_original_amazon\\sampled_original_amazon.json\"\n",
        "output_json_path = \"filtered_data.json\"\n",
        "stopwords_file = \"C:\\\\Users\\\\admin\\\\Documents\\\\english.txt\"\n",
        "\n",
        "# Define the columns to keep\n",
        "columns_to_keep = ['asin', 'title','price', 'also_buy', 'also_view']\n",
        "\n",
        "# Print statement for loading product data\n",
        "print(\"Filtering product data...\")\n",
        "\n",
        "# Read stopwords from the file\n",
        "stopwords = read_stopwords_from_file(stopwords_file)\n",
        "\n",
        "# Get total records in the JSON file\n",
        "total_records = get_total_records(input_json_path)\n",
        "\n",
        "# Calculate records per chunk based on total records\n",
        "records_per_chunk = min(total_records, 100000)  # Set a maximum of 100000 records per chunk\n",
        "\n",
        "# Read from input JSON file in chunks, filter, preprocess, and write to output JSON file\n",
        "with open(input_json_path, 'r') as input_file, open(output_json_path, 'w') as output_file:\n",
        "    process_data_in_chunks(input_file, output_file, total_records, records_per_chunk, stopwords)\n",
        "\n",
        "# Print statement indicating completion\n",
        "print(\"Filtered and preprocessed data saved to 'filtered_data.json'\")\n"
      ],
      "metadata": {
        "id": "SRp9iTDGLq3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}